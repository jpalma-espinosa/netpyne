---
title: "Ph.D. Project Proposal, v 2.0"
author: "Javier Palma Espinosa"
date: '`r paste(Sys.Date())`'
output:
  pdf_document:
    template: /home/javier/.pandoc/templates/eisvogel.latex
    toc: true
titlepage: true
titlepage-background: /home/javier/ImÃ¡genes/fondoRmd.png
csl: ../references/nature-neuroscience.csl
bibliography: ../rmd/bib-proposal.bib  


---
## Introduction


My work will be in exploring the computational mechanism and/or computational models of Selective Attention, with emphasis in the insect brain.  For that, I need to answer several questions:

1. What is known in selective attention in insects? see Arena [-@arena2012modeling]. This involves the state of the art.
2. What are the most relevant mechanisms in selective attention? Name models and mechanisms which sustain my hypothesis
3. Which neural structures are involved in selective attention? See Krauliz [-@krauzlis2018selective]

I am proposing that a _network with a bunch of attractors is a good candidate to sustain selective attention._ However, **What is the mechanism on which an input stimuli _drives_ the network towards one or another attractor?** See Miller [-@miller2016itinerancy].

In other words:

* having a network that responds with *x* to the stimuli "A", and responds with *y* to stimuli "B", when presenting "A+B": Why the network only outputs *x*? 
* What is the underlying mechanism that is driving the network towards that output? can I play with that kind of output?(neural control, biofeedback, ADHD????)
* What is the computational mechanism on which selective attention is performed in insects? 

## Hypothesis
Given the network architecture of the mushroom body, when a stimuli $x$ arrives, the network is driven towards certain attractor. **Selective attention is performed by controlling the trajectory(or the attractor strength) that drives the network towards that attractor.**


## Objectives

1. **LFP attractor analysis.** This will help me to determine wheter Mushrom body exhibit an attractor topology with and without attention. _For this, I MUST use Van Swinderen's data_ The working hypothesis here is that __I will indeed find attractors in the "resting state" model of the LFP and the network will transit to a stable state while the attention has been fixed.__ In here I could follow the trajectory of the system.
2. **Reconstruct and analyze the attractor network.** Here I will explore the computational mechanism of the reduced model of the MB.  This will give me space for studying the properties of the network and do a proper characterization of the attractors and the trajectories that the system should follow
3. **Test the selective attention in the built network.**
  
  > Having a network that responds with *x* to the stimuli "A", and responds with *y* to stimuli "B", when presenting "A+B", show that the network only outputs *x*"
  
4. **Controlability of the attractor.** Once the network has been build and analyzed, I will try several ideas to control the stability and settle of the network. There are some papers that propose a biologically plausible mechanism for this.
5. **Replicate the experiments.** In particular, I am thinking the work of Bruno's Lab [@grabowska2020oscillations;@paulk2014selective;@van2009shared]

## Preliminary results

### LFP attractor analysis {#lfp-analysis1}
The first idea to test was to reconstruct the phase space using Taken's embedding theorem.  For this, I was able to build a phase space for a simple signal: A 15-secs sinusoid. $f_1 = 10Hz$ for the first 7.5 seconds and $f_2 = 37Hz$ for the last 7.5 seconds.  The results of this excersise is shown in the figure \@ref(fig:attractor1)

```python
analysis(t,fs,signal)
#Results for the noiseless case
JIDT Analysis done in 40.7085 seconds. Dopt=3.00. Tauopt=1.00
JIDT Analysis done in 14.0192 seconds. Dopt=2.00. Tauopt=1.00
JIDT Analysis done in 18.4291 seconds. Dopt=10.00. Tauopt=50.00
```

```{r attractor1, echo=FALSE, fig.cap="\\textbf{Takens Reconstruction: Noiseless case.} \\textbf{Top} shows the signal in the range that the frequency is changed. \\textbf{Middle} shows the wavelet transform, which allow to look the frequency transition in the 7.5 seconds. \\textbf{Bottom} shows the reconstruction of the space phase. Left plot is the phase reconstruction for the whole signal.  Middle for the first portion, and right for the second portion of the signal", out.width = '100%', fig.align='center'}
knitr::include_graphics('../figures/attractor1.png')
```

After this exercise, I tried with an additive gaussian noise. 
```python
noise = np.random.rand(len(y)) #y is the signal
analysis(t,fs,y+noise)
#Results for noise analysis
JIDT Analysis done in 88.512 seconds. Dopt=10.00. Tauopt=18.00
JIDT Analysis done in 41.1819 seconds. Dopt=10.00. Tauopt=24.00
JIDT Analysis done in 38.4141 seconds. Dopt=10.00. Tauopt=20.00
```

```{r attractor2, echo=FALSE, fig.cap="\\textbf{Takens Reconstruction with small noise} Same plots as the noiseless case. Notice how the two attractors tend to merge if the whole signal is analyzed (bottom left plot) ", out.width = '100%', fig.align='center'}
knitr::include_graphics('../figures/attractor2.png')
```

Finally, I tried with a huge ammount of noise
```python
noise = np.random.rand(len(y)) #y is the signal
analysis(t,fs,y+30*noise)
#Results for noise analysis
JIDT Analysis done in 100.071 seconds. Dopt=4.00. Tauopt=29.00
JIDT Analysis done in 41.8212 seconds. Dopt=8.00. Tauopt=40.00
JIDT Analysis done in 43.8605 seconds. Dopt=5.00. Tauopt=21.00
```

```{r attractor3, echo=FALSE, fig.cap="\\textbf{Takens Reconstruction with big noise} Same plots as the noiseless case. Noise is so high that even the wavelet transform is unable to distinguish the two frequencies (middle plot). Also, phase space reconstruction failed (bottom plots)", out.width = '100%', fig.align='center'}
knitr::include_graphics('../figures/attractor3.png')
```

The quick conclusion with this is that in principle is possible to reconstruct the phase space with some noise. However, the big question is **Could this work also for stochastic processes, such as LFP?**[@oprisan2018cocaine]

# References